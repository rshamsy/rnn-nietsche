{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# from fastai.io import *\n",
    "# from fastai.conv_learner import *\n",
    "# from fastai.column_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/nietzsche')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600893"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = (((PATH/'nietzsche.txt').open()).read())\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert vocab to idx\n",
    "idxs = list(range(len(vocab)))\n",
    "idx_to_char = {idx:char for idx,char in enumerate(vocab)}\n",
    "char_to_idx = {char:idx for idx,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [char_to_idx[char] for char in data]\n",
    "len_data = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 72, 57, 19, 25]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]\n",
      " [5 6 7 8]]\n",
      "[[1 2 3 4]\n",
      " [5 6 7 8]]\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([1,2,3,4])\n",
    "\n",
    "# b = np.array([5,6,7,8])\n",
    "\n",
    "# d = (np.stack((a,b)))\n",
    "\n",
    "# print(d)\n",
    "# print(a[0:3])\n",
    "\n",
    "a = [1,2,3,4]\n",
    "\n",
    "b = [5,6,7,8]\n",
    "\n",
    "anp = np.array(a)\n",
    "bnp = np.array(b)\n",
    "c = []\n",
    "c.append(a)\n",
    "c.append(b)\n",
    "np.asarray(c)\n",
    "print(np.stack(c))\n",
    "d = np.stack((anp,bnp))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to create train batches and test batches\n",
    "def batchwise_data (data, bs = 1, bptt = 3, start=0):\n",
    "    \"\"\"\n",
    "    batchwise_data converts \n",
    "    data = 1D np array of indexed chars\n",
    "    bs - how many examples passed through network at once\n",
    "    bptt - number of time steps, or length of sequence. \n",
    "    start - position at which the sequence of the examples in the first \n",
    "        batch begins in the 'data' array \n",
    "    \"\"\"\n",
    "\n",
    "    len_data = len(data)\n",
    "    interm_size = int(len_data/bs)\n",
    "    num_minibatches = interm_size/bptt\n",
    "    interm_arrays = np.array_split(data,bs) # array (list) of arrays (lists)\n",
    "#     interm_arrays = np.stack(interm_arrays) # matrix - 1 list per row\n",
    "#     print(interm_arrays)\n",
    "\n",
    "    # creating an array of lists of data for each minibatch (x). \n",
    "    batches_list = list()\n",
    "\n",
    "    sequence_starts = list(range(start,len(interm_arrays[0]),bptt))\n",
    "\n",
    "    for sequence_start in sequence_starts:\n",
    "        batch_inputs = list()\n",
    "\n",
    "        for interm in interm_arrays:\n",
    "            seq = np.array(interm[sequence_start:sequence_start+bptt])\n",
    "            batch_inputs.append(seq)\n",
    "\n",
    "        np.asarray(batch_inputs)\n",
    "        batches_list.append(np.stack(batch_inputs))\n",
    "\n",
    "#     print(xbatches_list)\n",
    "#     print(xbatches_list[0])\n",
    "\n",
    "    return batches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create small dataset so it is easier to debug\n",
    "smallsz = 2000\n",
    "datasm = data[:smallsz]\n",
    "\n",
    "# creating an array of lists of outputs for each minibatch (y). \n",
    "xbatches_list = batchwise_data(datasm, bs=2)\n",
    "ybatches_list = batchwise_data(datasm,bs=2,start=1)\n",
    "\n",
    "# to drop any remainders of batching\n",
    "# ybatches_list=ybatches_list[:-1]\n",
    "# xbatches_list=xbatches_list[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply conditions to make it reproducible:\n",
    "def align(xbatches, ybatches):\n",
    "    \"\"\"\n",
    "    both inputs as a list of np arrays\n",
    "    \"\"\"\n",
    "    lenx = len(xbatches)\n",
    "    leny = len(ybatches)\n",
    "    if lenx>leny:\n",
    "        for i in range(lenx-leny):\n",
    "            xbatches = xbatches[:-1]\n",
    "    \n",
    "    elif leny>lenx:\n",
    "        for i in range(leny-lenx):\n",
    "            ybatches = ybatches[:-1]\n",
    "            \n",
    "    \n",
    "    return xbatches,ybatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "333\n"
     ]
    }
   ],
   "source": [
    "print(len(xbatches_list))\n",
    "print(len(ybatches_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatches_list,ybatches_list = align(xbatches_list,ybatches_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 5, 43, 63],\n",
      "       [56, 14, 40]]), array([[32, 46, 82],\n",
      "       [63, 66, 77]])]\n",
      "[array([[43, 63, 32],\n",
      "       [14, 40, 63]]), array([[46, 82, 71],\n",
      "       [66, 77, 14]])]\n"
     ]
    }
   ],
   "source": [
    "print(xbatches_list[-2:])\n",
    "print(ybatches_list[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vocab = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def intlabel_ohencoding(array):\n",
    "    \n",
    "    \"\"\"\n",
    "    purpose of this method is to convert array's values\n",
    "    into onehot encoded values based on the number of \n",
    "    categories present exclusively in array\n",
    "    \n",
    "    array - must be integer label encoded already\n",
    "    \n",
    "    returns a list of onehot encoded data examples to be \n",
    "        reshaped into original sizes ie \n",
    "        (dim1,dim2,dim3,...,dim(n-1),-1)\n",
    "    \"\"\"\n",
    "    # data already in integers so don't need sklearn LabelEncoder, just OneHotEncoder\n",
    "    shape = array.shape\n",
    "    flat_size=1\n",
    "    for i in shape:\n",
    "        flat_size*=i\n",
    "\n",
    "    arr_flat = array.reshape((flat_size),1)\n",
    "\n",
    "    # Create an encoder instance, fit on data, and transform int labels\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(arr_flat)\n",
    "    onehotlabels_arrflat = enc.transform(arr_flat).toarray()\n",
    "    \n",
    "    # reshape back to original with additional dimension for encoded array\n",
    "    onehotlabels = np.reshape(onehotlabels_arrflat,(*array.shape,-1))\n",
    "    return onehotlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting y-values to one hot encodings \n",
    "\"\"\"\n",
    "using sklearn's onehotencoder for its efficiencies\n",
    "\"\"\"\n",
    "\n",
    "ybatches_arr = np.stack(ybatches_list)\n",
    "ybatches_onehot = intlabel_ohencoding(ybatches_arr)\n",
    "ybatches_onehot=np.reshape(ybatches_onehot,(*ybatches_arr.shape,-1))\n",
    "# ybatches_onehot[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three char model therefore need lists of every first, second, third and fourth char. \n",
    "op = 3\n",
    "xt1 = list(np.array(data)[range(0,len(data),op+1)])\n",
    "xt2 = list(np.array(data)[range(1,len(data),op+1)])\n",
    "xt3 = list(np.array(data)[range(2,len(data),op+1)])\n",
    "y = list(np.array(data)[range(3,len(data),op+1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN Model (Numpy) Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input data form - old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " [11, 46, 22, 11, 62],\n",
       " 150224,\n",
       " [74, 17, 22, 11, 9],\n",
       " 150223,\n",
       " [14, 14, 19, 36, 31],\n",
       " 150223,\n",
       " [53, 22, 33, 19, 77],\n",
       " 150223)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op, xt1[:5], len(xt1), xt2[:5], len(xt2), xt3[:5], len(xt3), y[:5], len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([11, 74, 14, 53, 46, 17, 14, 22, 22, 22], 600893)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10], len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 3 # the number of time steps, since output of 3rd time step is the prediction. \n",
    "x_train=np.stack([xt1[1:],xt2,xt3],1)\n",
    "y_train=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 300\n",
    "len(np.array_split(x_train,int(len(x_train)/bs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150223, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input data form - updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 2, 3) (333, 2, 3, 52)\n"
     ]
    }
   ],
   "source": [
    "# Will have to loop over to access char for a certian time step.\n",
    "x = np.asarray(xbatches_list)\n",
    "y = ybatches_onehot\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(array):\n",
    "    # assumes batch dimension is the first\n",
    "    arr_exp = np.exp(array)\n",
    "    sum_example = np.sum(arr_exp, axis = 1)\n",
    "    return arr_exp/sum_example[:1]\n",
    "    \n",
    "# test def\n",
    "#softmax(np.random.randint(-10,10,(10,5)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyRNN():\n",
    "    \n",
    "    def __init__(self, x, y, nhidden, bs, nvocab): # forget nfac for embeddings, for simplicity.\n",
    "        self.h = np.zeros(bs,nhidden)\n",
    "        self.U = np.random.uniform(low=-0.01,high=0.01,size=((nhidden+1),nhidden)) #why nhidden+1?\n",
    "        self.V = np.random.uniform(low=-0.01,high=0.01,size=(nhidden,nvocab))\n",
    "        self.W = np.eye(nhidden)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        \"\"\"\n",
    "        x assumed to be array of sequential inputs for one bptt - size: bs x bptt. Each time\n",
    "        step's input should therefore have bs rows and 1 col. \n",
    "        Form in which x is passed is assumed to be 3 columns vectors, each for each time step\n",
    "        \n",
    "        \"\"\"\n",
    "        bptt = len(x) # will give number of time-steps\n",
    "        o = [] # list to store output at each time step\n",
    "        hs = [self.h] # list to store hidden state after each time step \n",
    "        \n",
    "        for i in range(bptt):\n",
    "            x_in = x[:,i]\n",
    "            cat_in = np.concatenate((self.h,x_in),-1) # resultant size = bs x (nhidden+1)\n",
    "            cat_in_U = np.tanh(np.dot(cat_in,self.U)) # bs x nhidden\n",
    "            # should ideally have relu here, but for simplicity, not include\n",
    "            self.h = np.tanh(np.dot(cat_in_U, W)) # bs x nhidden\n",
    "            hs.append(self.h)      \n",
    "            out_presoft = np.tanh(np.dot(self.h,self.V)) # bs x nvocab      \n",
    "            #do softmax\n",
    "            out = softmax(out_presoft)    \n",
    "            o.append(out)\n",
    "\n",
    "        return o, h\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y_hat): #y_hat is the actual/true value of prediction; size - bs x 1\n",
    "    \n",
    "    #CHECK ALL OPS' DIMENSIONS\n",
    "    \n",
    "    dV = np.zeros((nhidden,nvocab)) # dLdV\n",
    "    dU = np.zeros((nhidden+1),nhidden) # dLdU\n",
    "    dW = np.zeros((nhidden,nhidden)) # dLdW\n",
    "    \n",
    "    o, h = self.forward(x)\n",
    "    T = len(h) #number of time steps\n",
    "    \n",
    "    dV__dV = 1\n",
    "    dV_m_tanh__dV = (np.tanh(np.tanh(np.dot(h[-1],self.W)))\n",
    "    dtanh__dV_m_tanh = (1-np.dot(dV_m_tanh__dV,self.V)**2)\n",
    "    Si = softmax(np.tanh(np.dot(self.V,dV_m_tanh__dV))) # bs x nvocab\n",
    "    Sj = softmax((np.dot(self.V,dV_m_tanh__dV))) # bs x nvocab\n",
    "    dout__doutpresoft = Si #will need a 3 dimensional array\n",
    "    \n",
    "    dE__dV = (1-(self.V*(np.tanh(np.tanh(self.h))))**2)*(np.tanh(np.tanh(self.W*self.h)))\n",
    "    \n",
    "    # softmax der https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    \n",
    "    \n",
    "    return dU,dV,dW\n",
    "    \n",
    "    \n",
    "NumpyRNN.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Specs\n",
    "* Ignored embeddings for simplicity - taking letters as indices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN Model - PyTorch - incomplete\n",
    "Will implement the following characteristics: \n",
    "* Concatenation of previous time-step's hidden state and next input\n",
    "* Instantiate all weight matrices (U, V, W) using random numbers initially - change to identity matrix later \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeCharRNN(nn.Module):\n",
    "    def __init__(self, nvocab, nfac, nhidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.e = nn.Embedding(nvocab, nfac)\n",
    "        self.e.weight.data.uniform_(-0.01,0.01) # must actually use kaiming he initialization guidelines. \n",
    "        \n",
    "        self.lin_in = nn.Linear(nhidden,(nhidden+nfac))\n",
    "        self.lin_in.weight.data.uniform_(-0.01,0.01)\n",
    "        self.lin_hidden = nn.Linear(nhidden,nhidden)\n",
    "        self.lin_hidden.weight.data.uniform_(-0.01,0.01)\n",
    "        self.lin_out = nn.Linear(nhidden,nvocab)\n",
    "        self.lin_out.weight.data.uniform_(-0.01,0.01)\n",
    "        \n",
    "    def forward(self,*cs):\n",
    "        \n",
    "        h = torch.zeroes(bs,nhidden)\n",
    "        for c in cs:\n",
    "            xin = self.e(c)\n",
    "            cat_in = torch.cat(h,xin,dim=1)\n",
    "            h = F.tanh(self.lin_hidden(self.lin_in(cat_in))) # or ReLU??\n",
    "        \n",
    "        return F.log_softmax(self.lin_out(h))\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Need to do to above class still\n",
    "* consolidate dimensions\n",
    "* ensure initialization is actually needed\n",
    "* chekc computation of non-linear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(idxs)-op-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU PyTorch Workflow - incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
